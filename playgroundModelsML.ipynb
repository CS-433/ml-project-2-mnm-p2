{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "playgroundModelsML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlwps_eNm3RP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0242148-861f-4572-894d-ee1684b1f664"
      },
      "source": [
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import os\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as transform\n",
        "from torchvision.utils import save_image\n",
        "import torch.functional as F\n",
        "import PIL\n",
        "import torch.utils.data as data\n",
        "import glob\n",
        "import random\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "DATA_FOLDER = '/content/drive/MyDrive/ml/data/'\n",
        "TRAIN_FOLDER = DATA_FOLDER + 'training/training/'\n",
        "GROUNDTRUTH_FOLDER = TRAIN_FOLDER + '/groundtruth/'\n",
        "TRAIN_IMAGES = TRAIN_FOLDER + 'images/'\n",
        "TEST_IMAGES = DATA_FOLDER + 'test_set_images/test_set_images/'\n",
        "AUGMENTED_FOLDER = DATA_FOLDER + 'augmented/'\n",
        "AUGMENTED_IMAGES = DATA_FOLDER + 'augmented/images/'\n",
        "AUGMENTED_GT = DATA_FOLDER + 'augmented/labels/'\n",
        "\n",
        "\n",
        "unet_pretrained_model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "    in_channels=3, out_channels=1, init_features=32, pretrained=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knTXzU37_SEw"
      },
      "source": [
        "\n",
        "#This dataloder allows to import the immages on by one, only the directories are in the RAM\n",
        "class DataLoaderSegmentation(data.Dataset):\n",
        "    #Initalize the dataloader. Instead of having all the pics in ram we only save the directories.\n",
        "    def __init__(self, folder_path):\n",
        "        super(DataLoaderSegmentation, self).__init__()\n",
        "        self.folder_path = folder_path\n",
        "        self.currentidx = 0\n",
        "        self.img_files = glob.glob(os.path.join(folder_path,'images/','*.png'))\n",
        "        self.mask_files = []\n",
        "        for img_path in self.img_files:\n",
        "             self.mask_files.append(os.path.join(folder_path,'labels',os.path.basename(img_path))) \n",
        "    #Return only the pic,label at specified index. The returned object is a torch tensor [channels,H,W]\n",
        "    def __getitem__(self, index):\n",
        "        if index >= len(self):\n",
        "          index = (index%len(self))\n",
        "        img_path = self.img_files[index]\n",
        "        mask_path = self.mask_files[index]\n",
        "        data = transform.functional.pil_to_tensor(PIL.Image.open(img_path)).type(torch.FloatTensor)/255\n",
        "        label =transform.functional.pil_to_tensor(PIL.Image.open(mask_path))[:1,:,:].type(torch.FloatTensor)/255\n",
        "        return (data,label)\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "    #Return a batch of size batch_size of pic,label from hte currentidx to currentidx + batch_size.\n",
        "    #The returned object is a torch tensor [batch_size,channels,H,W]\n",
        "    def next(self,batch_size = 1):\n",
        "        if batch_size == 1:\n",
        "          self.currentidx += batch_size\n",
        "          return self[self.currentidx-1]\n",
        "        else:\n",
        "          seq = [self[self.currentidx+i] for i in range(batch_size)]\n",
        "          self.currentidx += batch_size\n",
        "          imgs = [i[0] for i in seq]\n",
        "          lbls = [i[1] for i in seq]\n",
        "          \n",
        "          res = (torch.stack(imgs),torch.stack(lbls))\n",
        "          return res\n",
        "\n",
        "    def get_epoch(self):\n",
        "      return int(self.currentidx/len(self.img_files))\n",
        "\n",
        "    def split_train_test( self,seed = 1 ,test_portion = 0.2):\n",
        "\n",
        "      folder_path = self.folder_path\n",
        "\n",
        "      test_set = DataLoaderSegmentation(folder_path)\n",
        "      train_set = DataLoaderSegmentation(folder_path)\n",
        "\n",
        "      copy_img_files = self.img_files\n",
        "      copy_mask_files = self.mask_files\n",
        "\n",
        "      index_array = list(range(len(copy_img_files)))\n",
        "      random.Random(seed).shuffle(index_array)\n",
        "     \n",
        "      copy_img_files =  list(map(copy_img_files.__getitem__, index_array))\n",
        "      copy_mask_files = list(map(copy_mask_files.__getitem__, index_array))\n",
        "\n",
        "      quantity_test = int(len(copy_img_files)*test_portion)\n",
        "\n",
        "      test_set.img_files = copy_img_files[:quantity_test]\n",
        "      test_set.mask_files = copy_mask_files[:quantity_test]\n",
        "      \n",
        "      train_set.img_files = copy_img_files[quantity_test:]\n",
        "      train_set.mask_files = copy_mask_files[quantity_test:]\n",
        "      return train_set,test_set\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ-_8qJj_kNW"
      },
      "source": [
        "#Function that allows to augment the data in a customized fashion.\n",
        "dataset = DataLoaderSegmentation(folder_path = AUGMENTED_FOLDER)\n",
        "#img,label = dataset.next(batch_size= 1)\n",
        "#print(label.shape)\n",
        "#img,label = dataset.next(batch_size=4)\n",
        "#print(label.shape)\n",
        "#dataset.currentidx = 1\n",
        "#print(img.max())\n",
        "#print(label.max())\n",
        "train_set,test_set = dataset.split_train_test()\n",
        "#img,label = train_set.next(batch_size= 1)\n",
        "#print(label.max())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O40mlfn51bJP"
      },
      "source": [
        "class Unet(nn.Module):\n",
        "  def __init__(self,unet,add_dropouts = False):\n",
        "    super().__init__()\n",
        "    self.encoder1 = unet.encoder1\n",
        "    self.pool1 = unet.pool1\n",
        "    self.encoder2 = unet.encoder2\n",
        "    self.pool2 = unet.pool2\n",
        "    self.encoder3 = unet.encoder3\n",
        "    self.pool3 = unet.pool3\n",
        "    self.encoder4 = unet.encoder4\n",
        "    self.pool4 = unet.pool4\n",
        "\n",
        "    self.bottleneck = unet.bottleneck\n",
        "    \n",
        "    self.upconv4 = unet.upconv4\n",
        "    self.decoder4 = unet.decoder4\n",
        "    self.upconv3 = unet.upconv3\n",
        "    self.decoder3 = unet.decoder3\n",
        "    self.upconv2 = unet.upconv2\n",
        "    self.decoder2 = unet.decoder2\n",
        "    self.upconv1 = unet.upconv1\n",
        "    self.decoder1 = unet.decoder1\n",
        "\n",
        "    self.conv = unet.conv\n",
        "\n",
        "  def add_dropout_encoder(probability , encoder):\n",
        "    children = list(encoder.children())\n",
        "    conv2d_1 = children[0]\n",
        "    bn_1 = children[1]\n",
        "    relu_1 = children[2]\n",
        "    conv2d_2 = children[3]\n",
        "    bn_2 = children[4]\n",
        "    relu_2 = children[5]\n",
        "    dropout = nn.Dropout2d(p = probability)\n",
        "    return nn.Sequential(dropout,conv2d_1,bn_1,relu_1,dropout,conv2d_2,bn_2,relu_2)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    e1 = self.encoder1(x)\n",
        "\n",
        "    e2 = self.encoder2(self.pool1(e1))\n",
        "\n",
        "    e3 = self.encoder3(self.pool2(e2))\n",
        "\n",
        "    e4 = self.encoder4(self.pool3(e3))\n",
        "\n",
        "    bneck = self.bottleneck(self.pool4(e4))\n",
        "\n",
        "    d4 = self.upconv4(bneck)\n",
        "    d4 = torch.cat((d4,e4),dim = 1)\n",
        "    d4 = self.decoder4(d4)\n",
        "\n",
        "    d3 = self.upconv3(d4)\n",
        "    d3 = torch.cat((d3,e3),dim = 1)\n",
        "    d3 = self.decoder3(d3)\n",
        "\n",
        "    d2 = self.upconv2(d3)\n",
        "    d2 = torch.cat((d2,e2),dim = 1)\n",
        "    d2 = self.decoder2(d2)\n",
        "\n",
        "    d1 = self.upconv1(d2)\n",
        "    d1 = torch.cat((d1,e1),dim = 1)\n",
        "    d1 = self.decoder1(d1)\n",
        "\n",
        "    res = torch.sigmoid(self.conv(d1))\n",
        "\n",
        "    return res\n",
        "\n",
        "class Unet_with_aux_loss(nn.Module):\n",
        "  def __init__(self,unet,add_dropouts = False):\n",
        "    super().__init__()\n",
        "    self.encoder1 = unet.encoder1\n",
        "    self.pool1 = unet.pool1\n",
        "    self.encoder2 = unet.encoder2\n",
        "    self.pool2 = unet.pool2\n",
        "    self.encoder3 = unet.encoder3\n",
        "    self.pool3 = unet.pool3\n",
        "    self.encoder4 = unet.encoder4\n",
        "    self.pool4 = unet.pool4\n",
        "\n",
        "    self.bottleneck = unet.bottleneck\n",
        "    \n",
        "    self.upconv4 = unet.upconv4\n",
        "    self.decoder4 = unet.decoder4\n",
        "    self.upconv3 = unet.upconv3\n",
        "    self.decoder3 = unet.decoder3\n",
        "    self.upconv2 = unet.upconv2\n",
        "    self.decoder2 = unet.decoder2\n",
        "    self.upconv1 = unet.upconv1\n",
        "    self.decoder1 = unet.decoder1\n",
        "\n",
        "    self.conv = unet.conv\n",
        "    self.poolLayer = nn.AvgPool2d(kernel_size = (16,16), stride= (16,16), ceil_mode = False)\n",
        "\n",
        "  # def add_dropout_encoder(probability , encoder):\n",
        "  #   children = list(encoder.children())\n",
        "  #   conv2d_1 = children[0]\n",
        "  #   bn_1 = children[1]\n",
        "  #   relu_1 = children[2]\n",
        "  #   conv2d_2 = children[3]\n",
        "  #   bn_2 = children[4]\n",
        "  #   relu_2 = children[5]\n",
        "  #   dropout = nn.Dropout2d(p = probability)\n",
        "  #   return nn.Sequential(dropout,conv2d_1,bn_1,relu_1,dropout,conv2d_2,bn_2,relu_2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    e1 = self.encoder1(x)\n",
        "\n",
        "    e2 = self.encoder2(self.pool1(e1))\n",
        "\n",
        "    e3 = self.encoder3(self.pool2(e2))\n",
        "  \n",
        "    e4 = self.encoder4(self.pool3(e3))\n",
        "\n",
        "    bneck = self.bottleneck(self.pool4(e4))\n",
        "\n",
        "    d4 = self.upconv4(bneck)\n",
        "    d4 = torch.cat((d4,e4),dim = 1)\n",
        "    d4 = self.decoder4(d4)\n",
        "\n",
        "    d3 = self.upconv3(d4)\n",
        "    d3 = torch.cat((d3,e3),dim = 1)\n",
        "    d3 = self.decoder3(d3)\n",
        "\n",
        "    d2 = self.upconv2(d3)\n",
        "    d2 = torch.cat((d2,e2),dim = 1)\n",
        "    d2 = self.decoder2(d2)\n",
        "\n",
        "    d1 = self.upconv1(d2)\n",
        "    d1 = torch.cat((d1,e1),dim = 1)\n",
        "    d1 = self.decoder1(d1)\n",
        "\n",
        "    res = torch.sigmoid(self.conv(d1))\n",
        "\n",
        "    pooled_res = self.poolLayer(res)\n",
        "\n",
        "    return res,pooled_res"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97bQT-XN9LI7",
        "outputId": "1f8b117f-20cc-4259-c8d8-2e3c9ff239d5"
      },
      "source": [
        "#model = Unet(unet_pretrained_model)\n",
        "model = Unet_with_aux_loss(unet_pretrained_model)\n",
        "\n",
        "##model_aux.eval()\n",
        "#model.load_state_dict(torch.load('/content/drive/MyDrive/ml/results/unet_aux_loss'))\n",
        "model.to(device)\n",
        "#unet_aux_loss = Unet_with_aux_loss(model)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unet_with_aux_loss(\n",
              "  (encoder1): Sequential(\n",
              "    (enc1conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc1relu1): ReLU(inplace=True)\n",
              "    (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc1relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder2): Sequential(\n",
              "    (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc2relu1): ReLU(inplace=True)\n",
              "    (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc2relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder3): Sequential(\n",
              "    (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc3relu1): ReLU(inplace=True)\n",
              "    (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc3relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder4): Sequential(\n",
              "    (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc4relu1): ReLU(inplace=True)\n",
              "    (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc4relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (bottleneck): Sequential(\n",
              "    (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bottleneckrelu1): ReLU(inplace=True)\n",
              "    (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bottleneckrelu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder4): Sequential(\n",
              "    (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec4relu1): ReLU(inplace=True)\n",
              "    (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec4relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder3): Sequential(\n",
              "    (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec3relu1): ReLU(inplace=True)\n",
              "    (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec3relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder2): Sequential(\n",
              "    (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec2relu1): ReLU(inplace=True)\n",
              "    (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec2relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder1): Sequential(\n",
              "    (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec1relu1): ReLU(inplace=True)\n",
              "    (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec1relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (poolLayer): AvgPool2d(kernel_size=(16, 16), stride=(16, 16), padding=0)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU4BOyDsuW3J"
      },
      "source": [
        "import re\n",
        "def get_patches(gt):\n",
        "  rounded_res = torch.round(gt)\n",
        "  patches = []\n",
        "  mask_size = 16\n",
        "  for i in range(int(rounded_res.shape[2]/mask_size)):\n",
        "    for j in range(int(rounded_res.shape[2]/mask_size)):\n",
        "      mask = torch.zeros((400,400)).type(torch.bool)\n",
        "      mask[i*mask_size:(i+1)*mask_size,j*mask_size:(j+1)*mask_size] = True\n",
        "      mask = mask.unsqueeze(0)\n",
        "      patches.append(rounded_res[:,mask].reshape((rounded_res.shape[0],1,mask_size,mask_size)).mean())\n",
        "\n",
        "  return patches\n",
        "\n",
        "def load_images(img_folder,gt_folder,quantity,all = False):\n",
        "    \"\"\"load 'quantity images to 2 numpy array. satelite images and groundtruth images'\"\"\"\n",
        "    \n",
        "    #extract name of images in both folders\n",
        "    files_img = os.listdir(img_folder)\n",
        "    files_gt = os.listdir(gt_folder)\n",
        "    \n",
        "    files_img = sorted(files_img)\n",
        "    files_gt = sorted(files_gt)\n",
        "    #if we don't want all images extract only 'quantitiy' first images\n",
        "    print(files_img[1])\n",
        "    print(files_gt[1])\n",
        "    if not all:\n",
        "        files_img = files_img[:quantity]\n",
        "        files_gt = files_gt[:quantity]\n",
        "    \n",
        "    #list containing the satelite images\n",
        "    ls_img = []\n",
        "    \n",
        "    #extract satelite images\n",
        "    i = 0\n",
        "    for file in files_img:\n",
        "        img = mpimg.imread(img_folder+file)\n",
        "        ls_img.append(img)\n",
        "        i+=1\n",
        "        if i % 100 == 0:\n",
        "          print(i)\n",
        "        \n",
        "    #list containing ground truths\n",
        "    ls_gt = []\n",
        "    i = 0\n",
        "    #extract ground truths\n",
        "    for file in files_gt:\n",
        "        img = mpimg.imread(gt_folder+file)\n",
        "        ls_gt.append(img)\n",
        "        i+=1\n",
        "        if i % 100 == 0:\n",
        "          print(i)\n",
        "        \n",
        "    return np.asarray(ls_img),np.asarray(ls_gt)\n",
        "\n",
        "def load_test_images(img_folder):\n",
        "    \"\"\"load 'quantity images to 2 numpy array. satelite images and groundtruth images'\"\"\"\n",
        "    def get_number(x):\n",
        "      return int(re.sub(\"[^0-9]\", \"\", x))\n",
        "\n",
        "    #extract name of images in both folders\n",
        "    files_img = os.listdir(img_folder)\n",
        "    files_img.sort(key = get_number)\n",
        "\n",
        "    #list containing the satelite images\n",
        "    ls_img = []\n",
        "    \n",
        "    #extract satelite images\n",
        "    for file in files_img:\n",
        "        print(file)\n",
        "        img = mpimg.imread(img_folder+file+'/'+file + '.png')\n",
        "        ls_img.append(img)\n",
        "        \n",
        "    return np.asarray(ls_img)\n",
        "\n",
        "def showImage(images,gt,index,torch_format = False):\n",
        "    \"\"\"show an image and it's groundtruth. index indicates the index of the image in their respective arrays'\"\"\"\n",
        "    # Show first image and its groundtruth image\n",
        "    flag = False\n",
        "    if torch_format:\n",
        "      images,gt = torch_to_numpy_format(images,gt)\n",
        "      if images.shape[3] == 1:\n",
        "        images = images.squeeze(-1)\n",
        "        flag = True\n",
        "      \n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(images[index], cmap='Greys_r')\n",
        "    plt.show()\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(gt[index], cmap='Greys_r')\n",
        "    plt.show()\n",
        "    if torch_format:\n",
        "      if flag:\n",
        "        images =  np.expand_dims(images,-1)\n",
        "      images,gt = numpy_to_torch_format(images,gt)\n",
        "      \n",
        "    \n",
        "def shuffle_data(imgs,gt):\n",
        "    index_array = np.arange(imgs.shape[0])\n",
        "    np.random.shuffle(index_array)\n",
        "    imgs = imgs[index_array]\n",
        "    gt = gt[index_array]\n",
        "    return imgs,gt\n",
        "\n",
        "def turn_gt_to_torch_float(gt):\n",
        "  return torch.from_numpy(np.round(gt)).type(torch.LongTensor).float()\n",
        "\n",
        "def numpy_to_torch_format(images,gt):\n",
        "  return torch.from_numpy(np.moveaxis(images,3,1)),turn_gt_to_torch_float(gt).unsqueeze(1)\n",
        "\n",
        "def torch_to_numpy_format(images,gt):\n",
        "  return np.moveaxis(images.numpy(),1,3),gt.squeeze(1).numpy()\n",
        "\n",
        "def split_data(imgs,gt,test_proportion,patches = False):\n",
        "  imgs,gt = shuffle_data(imgs,gt)\n",
        "  test_size = int(imgs.shape[0]* test_proportion)\n",
        "  test_img = imgs[:test_size]\n",
        "  test_gt = gt[:test_size]\n",
        "  train_img = imgs[test_size:]\n",
        "  train_gt = gt[test_size:]\n",
        "\n",
        "  if patches:\n",
        "    patches_train = get_patches(train_gt)\n",
        "    patches_test = get_patches(test_gt)\n",
        "  else:\n",
        "    patches_train = []\n",
        "    patches_test = []\n",
        "  return train_img,train_gt,test_img,test_gt,patches_train,patches_test\n",
        "\n",
        "\n",
        "def resize_data(imgs,gt,new_size):\n",
        "    original_size = imgs.shape[1]\n",
        "    diff_size = (new_size - original_size)\n",
        "    \n",
        "    \n",
        "    if diff_size > 0:\n",
        "        extra_side_1 = int(diff_size/2) if diff_size % 2 == 0 else int(diff_size/2)+1\n",
        "        extra_side_2 = int(diff_size/2) \n",
        "        resized_img = np.zeros((imgs.shape[0],new_size,new_size,imgs.shape[3]))\n",
        "        resized_img[:,extra_side_1 : -extra_side_2,extra_side_1 : -extra_side_2,:] = imgs\n",
        "        \n",
        "        resized_gt= np.zeros((gt.shape[0],new_size,new_size))\n",
        "        resized_gt[:,extra_side_1 : -extra_side_2,extra_side_1 : -extra_side_2] = gt\n",
        "        \n",
        "    else:\n",
        "        extra_side_1 = int(abs(diff_size)/2) if diff_size % 2 == 0 else int(abs(diff_size)/2)+1\n",
        "        extra_side_2 = int(abs(diff_size)/2) \n",
        "        resized_img = imgs[:,extra_side_1 : -extra_side_2,extra_side_1 : -extra_side_2,:]\n",
        "        resized_gt = gt[:,extra_side_1 : -extra_side_2,extra_side_1 : -extra_side_2]\n",
        "        \n",
        "        \n",
        "    return resized_img,resized_gt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onYF0nc_om11"
      },
      "source": [
        "def train_model(model,train_set,test_set,optimizer,criterion,scheduler,\n",
        "                filename,preprocess_img,preprocess_label,mini_batch_size = 16,nb_epochs = 30, criterion2 = None,\n",
        "                use_scheduler = True, print_progress= True,aux_loss = False):\n",
        "\n",
        "    validation_loss = []\n",
        "    train_loss = []\n",
        "    validation_acc = []\n",
        "    train_acc = []\n",
        "    model.train()\n",
        "    batch_size = mini_batch_size\n",
        "    avg_pool = nn.AvgPool2d(kernel_size = (16,16), stride= (16,16), ceil_mode = False)\n",
        "    for e in range(nb_epochs):\n",
        "        acc_loss = 0\n",
        "        current_epoch = train_set.get_epoch()\n",
        "        nb_batches_train = 0\n",
        "        while train_set.get_epoch() == current_epoch:\n",
        "            nb_batches_train += 1\n",
        "\n",
        "            input,train_target = train_set.next(batch_size = batch_size)\n",
        "\n",
        "            input = preprocess_img(input)\n",
        "           \n",
        "            train_target = preprocess_label(train_target)\n",
        "           \n",
        "            \n",
        "            if aux_loss: \n",
        "              output,pooled_output = model(input.to(device))\n",
        "\n",
        "              loss = criterion(output,train_target.to(device))\n",
        "\n",
        "              pooled_train_target = avg_pool(train_target)\n",
        "\n",
        "              pooled_train_target =  preprocess_label_basic_unet(pooled_train_target,threshold = 0.25)\n",
        "\n",
        "              loss += criterion2(pooled_output,pooled_train_target.to(device))*625\n",
        "\n",
        "              acc_loss = acc_loss + loss.item()\n",
        "\n",
        "            else:\n",
        "              output = model(input.to(device))\n",
        "\n",
        "              loss = criterion(output, train_target.to(device))\n",
        "\n",
        "              acc_loss = acc_loss + loss.item()\n",
        "\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if(print_progress):\n",
        "          ##loss\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                val_loss = 0.0\n",
        "                nb_batches_test = 0\n",
        "                current_epoch = test_set.get_epoch()\n",
        "                while test_set.get_epoch() == current_epoch:\n",
        "                  nb_batches_test += 1\n",
        "                  input,test_target = test_set.next(batch_size = batch_size)\n",
        "\n",
        "                  input = preprocess_img(input)\n",
        "                  if aux_loss: \n",
        "                    output,pooled_output = model(input.to(device))\n",
        "\n",
        "                    loss1 = criterion(output, test_target.to(device))\n",
        "\n",
        "                    pooled_test_target = avg_pool(test_target)\n",
        "                    pooled_test_target = preprocess_label_basic_unet(pooled_test_target,threshold = 0.25)\n",
        "\n",
        "                    loss += criterion2(pooled_output,pooled_test_target.to(device))*625\n",
        "\n",
        "                    val_loss = val_loss + loss.item() \n",
        "                  else:\n",
        "                    output = model(input.to(device))\n",
        "                    \n",
        "                    loss = criterion(output, test_target.to(device))\n",
        "                    \n",
        "                    val_loss = val_loss + loss.item()\n",
        "\n",
        "               \n",
        "                print(e,'train_loss: ' ,acc_loss/nb_batches_train)\n",
        "                train_loss.append(acc_loss/nb_batches_train)\n",
        "\n",
        "                \n",
        "                validation_loss.append(val_loss/nb_batches_test)\n",
        "                print('val_loss: ', val_loss/nb_batches_test)\n",
        "                print()\n",
        "        model.train()\n",
        "        if use_scheduler:\n",
        "          scheduler.step(validation_loss[-1])\n",
        "        torch.save(model.state_dict(), filename)\n",
        "    return train_loss,validation_loss,train_acc,validation_acc"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZUZNsxgpADX"
      },
      "source": [
        "def compute_mean_std(dataset):\n",
        "  \n",
        "  \n",
        "  img,_ = dataset.next(batch_size=1)\n",
        "  dataset.currentidx = 0\n",
        "  current_epoch = dataset.get_epoch()\n",
        "  mean = torch.zeros(img.shape)\n",
        "  std = torch.zeros(img.shape)\n",
        "  print('computing_mean')\n",
        "  while dataset.get_epoch() == current_epoch:\n",
        "    img,_ = dataset.next(batch_size=1)\n",
        "    mean += img\n",
        "  mean /= len(dataset)\n",
        "  print('computing_std')\n",
        "  current_epoch = dataset.get_epoch()\n",
        "  while dataset.get_epoch() == current_epoch:\n",
        "    img,_ = dataset.next(batch_size=1)\n",
        "    std += torch.square(img-mean)\n",
        "  \n",
        "  std = torch.sqrt(std/len(dataset))\n",
        "  return mean,std \n",
        "\n",
        "\n",
        "def preprocess_label_basic_unet(image,threshold = 0.5):\n",
        "  image[image >= threshold] = 1\n",
        "  image[image < threshold] = 0\n",
        "  return image\n",
        "\n",
        "\n",
        "def compute_pos_weight_matrix(dataset,aux_loss = False):\n",
        "  \n",
        "  if not aux_loss:\n",
        "    _,label = dataset.next(batch_size=1)\n",
        "    dataset.currentidx = 0\n",
        "    current_epoch = dataset.get_epoch()\n",
        "    pos_nb = torch.zeros(label.shape)\n",
        "    neg_nb = torch.zeros(label.shape)\n",
        "    while dataset.get_epoch() == current_epoch:\n",
        "      _,label = dataset.next(batch_size=2)\n",
        "\n",
        "      label = preprocess_label_basic_unet(label)\n",
        "      neg_nb  += (label == 0).sum(dim = 0)\n",
        "      pos_nb += (label ==1).sum(dim = 0)\n",
        "      \n",
        "    pos_weight = neg_nb/pos_nb\n",
        "  else:\n",
        "    avg_pool = nn.AvgPool2d(kernel_size = (16,16), stride= (16,16), ceil_mode = False)\n",
        "    _,label = dataset.next(batch_size=1)\n",
        "    label = avg_pool(label)\n",
        "    dataset.currentidx = 0\n",
        "    current_epoch = dataset.get_epoch()\n",
        "    pos_nb = torch.zeros(label.shape)\n",
        "    neg_nb = torch.zeros(label.shape)\n",
        "    while dataset.get_epoch() == current_epoch:\n",
        "      _,label = dataset.next(batch_size=2)\n",
        "      label = avg_pool(label)\n",
        "      label = preprocess_label_basic_unet(label,threshold = 0.25)\n",
        "      neg_nb  += (label == 0).sum(dim = 0)\n",
        "      pos_nb += (label ==1).sum(dim = 0)\n",
        "      \n",
        "    pos_weight = neg_nb/pos_nb\n",
        "  return pos_weight\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZfUq6b9W8EC",
        "outputId": "09f93963-c92a-4633-d511-852164691faa"
      },
      "source": [
        "####PIPELINE FOR BASIC UNET MODEL\n",
        "\n",
        "mean,std = compute_mean_std(train_set)\n",
        "preprocess_input = transforms.Compose([transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "model.train()\n",
        "pos_weight = compute_pos_weight_matrix(train_set,aux_loss = True)\n",
        "\n",
        "pos_weight= pos_weight.to(device)\n",
        "criterion2 = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computing_mean\n",
            "computing_std\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS1xaP2iHVes",
        "outputId": "bdcc0dd1-2110-4f69-909d-39c3b08f04c3"
      },
      "source": [
        "patch_criterion = torch.nn.MSELoss()\n",
        "model.train()\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.1)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience = 3,threshold= 0.1,threshold_mode = 'abs',verbose = True)\n",
        "train_loss,val_loss , _ , _ = train_model(model,train_set,test_set,optimizer,criterion,scheduler,\n",
        "                                          '/content/drive/MyDrive/ml/results/unet_aux_loss_2',preprocess_input,preprocess_label_basic_unet,mini_batch_size = 10,nb_epochs = 50, criterion2 = criterion2,\n",
        "                                          use_scheduler = True, print_progress= True,aux_loss = True)\n",
        "    \n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 train_loss:  622.2494932810465\n",
            "val_loss:  8131.797164916992\n",
            "\n",
            "1 train_loss:  598.9694226582845\n",
            "val_loss:  8048.453704833984\n",
            "\n",
            "2 train_loss:  579.3555800120035\n",
            "val_loss:  7828.34939066569\n",
            "\n",
            "3 train_loss:  566.4525241851807\n",
            "val_loss:  7535.625081380208\n",
            "\n",
            "4 train_loss:  551.8007857004801\n",
            "val_loss:  7509.36121114095\n",
            "\n",
            "5 train_loss:  544.0040887196859\n",
            "val_loss:  7360.1763916015625\n",
            "\n",
            "6 train_loss:  537.771910349528\n",
            "val_loss:  7196.161631266276\n",
            "\n",
            "7 train_loss:  530.8974405924479\n",
            "val_loss:  7152.845657348633\n",
            "\n",
            "8 train_loss:  527.7518711090088\n",
            "val_loss:  7101.177917480469\n",
            "\n",
            "9 train_loss:  523.2891050974528\n",
            "val_loss:  7057.737533569336\n",
            "\n",
            "10 train_loss:  518.7740163803101\n",
            "val_loss:  6969.650207519531\n",
            "\n",
            "11 train_loss:  515.8750836054484\n",
            "val_loss:  7037.922236124675\n",
            "\n",
            "12 train_loss:  513.8821954727173\n",
            "val_loss:  7172.766703287761\n",
            "\n",
            "13 train_loss:  511.4593280156453\n",
            "val_loss:  6975.870936075847\n",
            "\n",
            "14 train_loss:  510.31926409403485\n",
            "val_loss:  6919.063095092773\n",
            "\n",
            "15 train_loss:  508.6270395914714\n",
            "val_loss:  6925.168309529622\n",
            "\n",
            "16 train_loss:  507.39728323618573\n",
            "val_loss:  6918.530873616536\n",
            "\n",
            "17 train_loss:  506.5378122329712\n",
            "val_loss:  6982.698471069336\n",
            "\n",
            "18 train_loss:  505.64735953013104\n",
            "val_loss:  6874.80344136556\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bSE-ImR6o1Q"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ml/results/best_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4Bwtibs-bxo",
        "outputId": "6e670263-6de7-4bf9-c8e3-d8c2595e9f58"
      },
      "source": [
        "def get_stats(pred,act):\n",
        "  pred_0 = (pred == 0)\n",
        "  pred_1 = (pred == 1)\n",
        "  TN = (act[pred_0] == 0).sum()\n",
        "  TP = (act[pred_1] == 1).sum()\n",
        "  FN = (act[pred_0] == 1).sum()\n",
        "  FP = (act[pred_1] == 0).sum()\n",
        "  return TN,TP,FP,FN\n",
        "\n",
        "def get_f1_score(tn,tp,fp,fn):\n",
        "  precision = tp/(tp+fp)\n",
        "  recall = tp/(tp+fn)\n",
        "  return 2*precision*recall/(precision + recall)\n",
        "\n",
        "def find_best_threshold(test_img,test_gt,model,threshold_spacing):\n",
        "  model.eval()\n",
        "  f1_scores = []\n",
        "  thresholds = [i * threshold_spacing for i in range(int(1/threshold_spacing))]\n",
        "  \n",
        "  for threshold in thresholds:\n",
        "    TN_tot = 0\n",
        "    TP_tot  = 0\n",
        "    FP_tot = 0\n",
        "    FN_tot = 0\n",
        "    \n",
        "    for i in range(test_img.shape[0]):\n",
        "      predictions = model(test_img[i].unsqueeze(0).to(device)).to('cpu').detach()\n",
        "      predictions[predictions >= threshold] = 1\n",
        "      predictions[predictions < threshold] = 0\n",
        "      TN,TP,FP,FN = get_stats(predictions,test_gt[i])\n",
        "      TN_tot += TN\n",
        "      TP_tot += TP\n",
        "      FP_tot += FP\n",
        "      FN_tot += FN\n",
        "    f1_scores.append(get_f1_score(TN_tot,TP_tot,FP_tot,FN_tot))\n",
        "  print('best threshold:')\n",
        "  print((np.argmax(np.array(f1_scores))*threshold_spacing))\n",
        "  print('with f1_score: ')\n",
        "  print(np.max(np.array(f1_scores)))\n",
        "  for i in range(test_img.shape[0]):\n",
        "      predictions = model(test_img[i].unsqueeze(0).to(device)).to('cpu').detach()\n",
        "      predictions[predictions >= 0.5] = 1\n",
        "      predictions[predictions < 0.5] = 0\n",
        "      TN,TP,FP,FN = get_stats(predictions,test_gt[i])\n",
        "      TN_tot += TN\n",
        "      TP_tot += TP\n",
        "      FP_tot += FP\n",
        "      FN_tot += FN\n",
        "  print('vs 0.5 thresholds:')\n",
        "  print(get_f1_score(TN_tot,TP_tot,FP_tot,FN_tot))\n",
        "  return f1_scores\n",
        "\n",
        "model.eval()\n",
        "#test = find_best_threshold(test_img,test_gt,model,0.05)\n",
        "#train = find_best_threshold(train_img,train_gt,model,0.05)\n",
        "\n",
        "    \n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unet_with_aux_loss(\n",
              "  (encoder1): Sequential(\n",
              "    (enc1conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc1relu1): ReLU(inplace=True)\n",
              "    (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc1relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder2): Sequential(\n",
              "    (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc2relu1): ReLU(inplace=True)\n",
              "    (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc2relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder3): Sequential(\n",
              "    (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc3relu1): ReLU(inplace=True)\n",
              "    (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc3relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder4): Sequential(\n",
              "    (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc4relu1): ReLU(inplace=True)\n",
              "    (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc4relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (bottleneck): Sequential(\n",
              "    (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bottleneckrelu1): ReLU(inplace=True)\n",
              "    (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bottleneckrelu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder4): Sequential(\n",
              "    (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec4relu1): ReLU(inplace=True)\n",
              "    (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec4relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder3): Sequential(\n",
              "    (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec3relu1): ReLU(inplace=True)\n",
              "    (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec3relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder2): Sequential(\n",
              "    (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec2relu1): ReLU(inplace=True)\n",
              "    (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec2relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder1): Sequential(\n",
              "    (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec1relu1): ReLU(inplace=True)\n",
              "    (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec1relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (poolLayer): AvgPool2d(kernel_size=(16, 16), stride=(16, 16), padding=0)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats_images(data,batch_size,preprocess_img):\n",
        "  data.currentidx = 0\n",
        "  current_epoch = data.get_epoch()\n",
        "  avg_pool = nn.AvgPool2d(kernel_size = (16,16), stride= (16,16), ceil_mode = False)\n",
        "  TN_tot = 0\n",
        "  TP_tot = 0\n",
        "  FP_tot = 0\n",
        "  FN_tot = 0\n",
        "  f1_scores = []\n",
        "  while data.get_epoch() == current_epoch:\n",
        "    \n",
        "    input,test_target = data.next(batch_size = batch_size)\n",
        "\n",
        "    input = preprocess_img(input)\n",
        "\n",
        "    _,pooled_output = model(input.to(device))\n",
        "\n",
        "    \n",
        "    pooled_test_target = avg_pool(test_target)\n",
        "    pooled_test_target = preprocess_label_basic_unet(pooled_test_target,threshold = 0.25)\n",
        "\n",
        "    pooled_output = preprocess_label_basic_unet(pooled_output.detach().to('cpu'),threshold = 0.25)\n",
        "    TN,TP,FP,FN = get_stats(pooled_output,pooled_test_target)\n",
        "    TN_tot += TN\n",
        "    TP_tot += TP\n",
        "    FP_tot += FP\n",
        "    FN_tot += FN\n",
        "  return get_f1_score(TN_tot,TP_tot,FP_tot,FN_tot)\n",
        "\n",
        "get_stats_images(train_set,5,preprocess_input)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqL-T2KKMfqg",
        "outputId": "cfcb41ce-f911-4261-8507-b40651a71225"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9472)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "-lRT7Sz_imDu",
        "outputId": "02cc427a-c13a-4f94-a76f-390574859bf3"
      },
      "source": [
        "#test = list(map(lambda x: x.item(),test))\n",
        "#train = list(map(lambda x: x.item(),train))\n",
        "\n",
        "plt.plot(list(range(int(len(train)))),train,color = 'red')\n",
        "plt.plot(list(range(len(test))),test,color = 'blue')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ3HMdY_SDsU"
      },
      "source": [
        "model.eval()\n",
        "prev_i = 0\n",
        "for img_,gt_ in zip(test_img,test_gt):\n",
        "  predictions = model(img_.unsqueeze(0).to(device))\n",
        "  predictions = torch.round(predictions)\n",
        "  gts = torch.round(gt_)\n",
        "  print(gts.shape)\n",
        "  print('first set')\n",
        "  print('acc: ')\n",
        "  print((predictions.to('cpu')[0] == gts[0]).sum()/predictions.shape[2]**2)\n",
        "  all_black_guess = torch.zeros(gts.shape)\n",
        "  print((all_black_guess[0] == gts[0]).sum()/predictions.shape[2]**2)\n",
        "  print()\n",
        "  showImage(predictions.clone().to('cpu').detach(),gts,0,True)\n",
        "  if prev_i == 3:\n",
        "    break\n",
        "  prev_i+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3KJzf3o1RZu"
      },
      "source": [
        "model.eval()\n",
        "torch.save(model,DATA_FOLDER+'basic_unet')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4hxhMazNiFc",
        "outputId": "112291f1-bafc-41ea-cda5-b14dd7fc4419"
      },
      "source": [
        "train_img.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.4466)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "bAx0qvhyjDYb",
        "outputId": "f021811e-6cab-46c5-b7f9-e29e31cd3322"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(list(range(len(train_loss))),train_loss,color = 'red',label = 'train_loss')\n",
        "plt.plot(list(range(len(val_loss))),val_loss,color = 'blue',label = 'val_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9faf005a5c03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWEQPBJLPiIH",
        "outputId": "1937c8cb-a675-494c-f8a3-efd86c16fd81"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0.8400, 0.8400, 0.8300,  ..., 0.7900, 0.8000, 0.8200],\n",
              "         [0.8400, 0.8400, 0.8300,  ..., 0.8000, 0.8000, 0.8200],\n",
              "         [0.8400, 0.8400, 0.8300,  ..., 0.7800, 0.8000, 0.8200],\n",
              "         ...,\n",
              "         [0.8100, 0.8100, 0.8100,  ..., 0.8600, 0.8500, 0.8500],\n",
              "         [0.8000, 0.8000, 0.8100,  ..., 0.8600, 0.8500, 0.8500],\n",
              "         [0.8100, 0.8100, 0.8200,  ..., 0.8700, 0.8600, 0.8600]]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAJs6FlQsXEK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsi6kUB_qM5m",
        "outputId": "d86e0cd7-24c3-4fe2-a401-e4a3ea93e20a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj3mcdnpqNmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c293a784-7b1d-498f-e392-49db8da79337"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_1\n",
            "test_2\n",
            "test_3\n",
            "test_4\n",
            "test_5\n",
            "test_6\n",
            "test_7\n",
            "test_8\n",
            "test_9\n",
            "test_10\n",
            "test_11\n",
            "test_12\n",
            "test_13\n",
            "test_14\n",
            "test_15\n",
            "test_16\n",
            "test_17\n",
            "test_18\n",
            "test_19\n",
            "test_20\n",
            "test_21\n",
            "test_22\n",
            "test_23\n",
            "test_24\n",
            "test_25\n",
            "test_26\n",
            "test_27\n",
            "test_28\n",
            "test_29\n",
            "test_30\n",
            "test_31\n",
            "test_32\n",
            "test_33\n",
            "test_34\n",
            "test_35\n",
            "test_36\n",
            "test_37\n",
            "test_38\n",
            "test_39\n",
            "test_40\n",
            "test_41\n",
            "test_42\n",
            "test_43\n",
            "test_44\n",
            "test_45\n",
            "test_46\n",
            "test_47\n",
            "test_48\n",
            "test_49\n",
            "test_50\n",
            "torch.Size([50, 3, 608, 608])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BICd-iG4KZZc",
        "outputId": "13ade79b-3c18-474f-fdfd-bb58d99c0c5b"
      },
      "source": [
        "timg.std(),timg.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.9967), tensor(-3.6853e-09))"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fWSFVfG-Pio_",
        "outputId": "c9921910-9d57-4329-a272-faccf82d348f"
      },
      "source": [
        "DATA_FOLDER"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/ml/data/'"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0LWCSXHPNUK"
      },
      "source": [
        "from PIL import Image\n",
        "prediction_dir = '/content/drive/MyDrive/ml/predictions/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0-FsJpLJcXD"
      },
      "source": [
        "for i in range(0,50):\n",
        "  _,a = model(timg[i].unsqueeze(0).to(device)).to('cpu').detach()\n",
        "  threshold = 0.25\n",
        "  a[a >= threshold] = 1\n",
        "  a[a < threshold] = 0\n",
        "  a,_ = torch_to_numpy_format(a,torch.zeros(1,608,608))\n",
        "  #if i < 9:\n",
        "   # Image.fromarray(a[0,:,:,0].astype(np.uint8)).save(prediction_dir + \"prediction_00\" + str(i+1) + \".png\")\n",
        "  #else:\n",
        "   # Image.fromarray(a[0,:,:,0].astype(np.uint8)).save(prediction_dir + \"prediction_0\" + str(i+1) + \".png\")\n",
        "  a[a > 0] = 255\n",
        "  if i < 9 :\n",
        "    Image.fromarray(a[0,:,:,0].astype(np.uint8)).save(prediction_dir + \"prediction_00\" + str(i+1) + \".png\")\n",
        "  else:\n",
        "    Image.fromarray(a[0,:,:,0].astype(np.uint8)).save(prediction_dir + \"prediction_0\" + str(i+1) + \".png\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Thi3oW_qJuCq",
        "outputId": "4265d29f-23de-4ac8-f1da-5bc911ef6112"
      },
      "source": [
        "a,v = load_images(TRAIN_IMAGES,GROUNDTRUTH_FOLDER,3,False)\n",
        "v.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 400, 400)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmlVotjaK8Vp",
        "outputId": "5b0e8675-f19f-4b65-eaf0-c399713aef1d"
      },
      "source": [
        "a.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 400, 400, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2r35S_BMEkE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f6052e-2532-4f9e-fac1-5020531f7266"
      },
      "source": [
        "test_img.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([120, 3, 400, 400])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eal6CC7klnXy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}